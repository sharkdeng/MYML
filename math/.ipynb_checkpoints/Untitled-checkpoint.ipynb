{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 3]), tensor([ 4,  3,  2,  6,  8, 10]), tensor([2, 3])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [torch.LongTensor([1, 2, 3]), \n",
    "         torch.LongTensor([4, 3, 2, 6, 8, 10]), \n",
    "         torch.LongTensor([2, 3])]\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_sents = sorted(sents, key=lambda x: x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(object):\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def __next__(self):\n",
    "        print('goog')\n",
    "        \n",
    "    next = __next__\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    data = '../data/penn/'\n",
    "    emsize = 850 # size of word embeddings\n",
    "    nhid =  850 # number of hidden units per layer\n",
    "    nhidlast = 850 # number of hidden units for the last rnn layer\n",
    "    lr = 20 # initial learning rate\n",
    "    clip = 0.25 # gradient clipping\n",
    "    epochs = 8000 # upper epoch limit\n",
    "    batch_size = 64 # batch size\n",
    "    bptt = 35 # sequence length\n",
    "    dropout = 0.75 # dropout applied to layers (0 = no dropout)\n",
    "    dropouth = 0.25 # dropout for hidden nodes in rnn layers (0 = no dropout)\n",
    "    dropoutx = 0.75 # dropout for input nodes rnn layers (0 = no dropout\n",
    "    dropouti = 0.2 # dropout for input embedding layers (0 = no dropout)\n",
    "    dropout3 = 0.1 # dropout to remove words from embedding layer (0 = no dropout)\n",
    "    seed = 1267 # random seed\n",
    "    nonmono = 5 # random seed\n",
    "    cuda = False # use CUDA\n",
    "    log_interval = 200 # report interval\n",
    "    save = 'EXP' # path to save the final model\n",
    "    alpha = 0 # alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\n",
    "    beta = 1e-3  # beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "    wdecay = 8e-7 # weight decay applied to all weights\n",
    "    continue_train = False # continue train from a checkpoint\n",
    "    small_batch_size = -1 # the batch size for computation. batch_size should be divisible by small_batch_size.\\\n",
    "                            # In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "                            # until batch_size is reached. An update step is then performed.\n",
    "    max_seq_len_detal = 20 # max sequence length\n",
    "    arch = 'DARTS' # which architecture to u se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 16, 3, 3)\n",
    "bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "conv2 = nn.Conv2d(16, 8, 3, 3)\n",
    "bn2 = nn.BatchNorm2d(8)\n",
    "\n",
    "conv3 = nn.Conv2d(8, 4, 3, 3)\n",
    "bn3 = nn.BatchNorm2d(4)\n",
    "\n",
    "maxpool = nn.MaxPool2d(1)\n",
    "flat = nn.Flatten()\n",
    "fc = nn.Linear(1296, 1)\n",
    "\n",
    "model = nn.Sequential(conv1, bn1, conv2, bn2, conv3, bn3, maxpool, flat, fc)\n",
    "model(torch.randn(12, 3, 512, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _construct_model_from_theta(self, theta):\n",
    "    model_new = self.model.new()\n",
    "    model_dict = self.model.state_dict()\n",
    "\n",
    "    params, offset = {}, 0\n",
    "    for k, v in self.model.named_parameters():\n",
    "      v_length = np.prod(v.size())\n",
    "      params[k] = theta[offset: offset+v_length].view(v.size())\n",
    "      offset += v_length\n",
    "\n",
    "    assert offset == len(theta)\n",
    "    model_dict.update(params)\n",
    "    model_new.load_state_dict(model_dict)\n",
    "    return model_new.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat(data):\n",
    "    return torch.cat([x.view(-1) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip(grads, max_norm):\n",
    "    \n",
    "    total_norm = 0\n",
    "    for g in grads:\n",
    "        param_norm = g.data.norm(2)\n",
    "        total_norm += param_norm ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    \n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for g in grads:\n",
    "            g.data.mul_(clip_coef)\n",
    "    return clip_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 16, 3, 3)\n",
    "bn1 = nn.BatchNorm2d(16)\n",
    "conv2 = nn.Conv2d(16, 8, 3, 3)\n",
    "bn2 = nn.BatchNorm2d(8)\n",
    "conv3 = nn.Conv2d(8, 4, 3, 3)\n",
    "bn3 = nn.BatchNorm2d(4)\n",
    "maxpool = nn.MaxPool2d(1)\n",
    "flat = nn.Flatten()\n",
    "fc = nn.Linear(1296, 1)\n",
    "\n",
    "model = nn.Sequential(conv1, bn1, conv2, bn2, conv3, bn3, maxpool, flat, fc)\n",
    "x = torch.randn(12, 3, 512, 512).float()\n",
    "y = torch.arange(12).float()\n",
    "loss_fn = nn.MSELoss()\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrolled_SGD(model, x, y, lr, network_clip=1, weight_decay=0.2):\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    \n",
    "    params = _concat(model.parameters()).data\n",
    "    \n",
    "    grads = torch.autograd.grad(loss, model.parameters())\n",
    "    clip_coef = _clip(grads, network_clip)\n",
    "    grads = _concat(grads).data + weight_decay*params\n",
    "    \n",
    "    unrolled_model = _construct_model_from_theta(params.sub(lr, grads))\n",
    "\n",
    "    return unrolled_model, clip_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _construct_model_from_theta(theta):\n",
    "    model_new = copy.deepcopy(model)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    params, offset = {}, 0\n",
    "    for k, v in model.named_parameters():\n",
    "        v_length = np.prod(v.size())\n",
    "        params[k] = theta[offset: offset+v_length].view(v.size())\n",
    "        offset += v_length\n",
    "\n",
    "    assert offset == len(theta)\n",
    "    model_dict.update(params) # if key exists, replace the value or if not, add the value\n",
    "    model_new.load_state_dict(model_dict)\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2022, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for a in model.parameters():\n",
    "    total += a.norm(2)**2\n",
    "total**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, coef = _compute_unrolled_model(model, x, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0789, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for a in new_model.parameters():\n",
    "    total += a.norm(2)**2\n",
    "total**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
